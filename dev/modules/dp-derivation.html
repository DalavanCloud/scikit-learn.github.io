
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>2.1.3.2.1. Variational Gaussian Mixture Models &mdash; scikit-learn 0.18.dev0 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.18.dev0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.18.dev0 documentation" href="../index.html" />
    <link rel="up" title="2.1. Gaussian mixture models" href="mixture.html" />
    <link rel="next" title="2.2. Manifold learning" href="manifold.html" />
    <link rel="prev" title="2.1. Gaussian mixture models" href="mixture.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/dp-derivation.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
  </script>

  </head>
  <body role="document">

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../install.html">Installation</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">Documentation</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn 0.18 (development)</li>
            <li><a href="../tutorial/index.html">Tutorials</a></li>
            <li><a href="../user_guide.html">User guide</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers/contributing.html">Contributing</a></li>
            <li class="divider"></li>
                <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.17 (stable)</a></li>
                <li><a href="http://scikit-learn.org/0.16/documentation.html">Scikit-learn 0.16</a></li>
                <li><a href="http://scikit-learn.org/0.15/documentation.html">Scikit-learn 0.15</a></li>
				<li><a href="../_downloads/scikit-learn-docs.pdf">PDF documentation</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            </ul>

            <div class="search_form">
                <div id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="mixture.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        2.1. Gaussian...
        </span>
            <span class="hiddenrellink">
            2.1. Gaussian mixture models
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="mixture.html">
        Up
        <br/>
        <span class="smallrellink">
        2.1. Gaussian...
        </span>
            <span class="hiddenrellink">
            2.1. Gaussian mixture models
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version">This documentation is for scikit-learn <strong>version 0.18.dev0</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    <p class="citing">If you use the software, please consider <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <ul>
<li><a class="reference internal" href="#">2.1.3.2.1. Variational Gaussian Mixture Models</a></li>
<li><a class="reference internal" href="#update-rules-for-vb-inference">2.1.3.2.2. Update rules for VB inference</a><ul>
<li><a class="reference internal" href="#the-spherical-model">2.1.3.2.2.1. The spherical model</a><ul>
<li><a class="reference internal" href="#the-bound">2.1.3.2.2.1.1. The bound</a></li>
<li><a class="reference internal" href="#the-updates">2.1.3.2.2.1.2. The updates</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-diagonal-model">2.1.3.2.2.2. The diagonal model</a><ul>
<li><a class="reference internal" href="#the-lower-bound">2.1.3.2.2.2.1. The lower bound</a></li>
<li><a class="reference internal" href="#id1">2.1.3.2.2.2.2. The updates</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-tied-model">2.1.3.2.2.3. The tied model</a><ul>
<li><a class="reference internal" href="#id2">2.1.3.2.2.3.1. The lower bound</a></li>
<li><a class="reference internal" href="#id3">2.1.3.2.2.3.2. The updates</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-full-model">2.1.3.2.2.4. The full model</a><ul>
<li><a class="reference internal" href="#id4">2.1.3.2.2.4.1. The lower bound</a></li>
<li><a class="reference internal" href="#id5">2.1.3.2.2.4.2. The updates</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="variational-gaussian-mixture-models">
<span id="dpgmm-derivation"></span><h1>2.1.3.2.1. Variational Gaussian Mixture Models<a class="headerlink" href="#variational-gaussian-mixture-models" title="Permalink to this headline">Â¶</a></h1>
<p>The API is identical to that of the <a class="reference internal" href="generated/sklearn.mixture.GMM.html#sklearn.mixture.GMM" title="sklearn.mixture.GMM"><code class="xref py py-class docutils literal"><span class="pre">GMM</span></code></a> class, the main
difference being that it offers access to precision matrices as well
as covariance matrices.</p>
<p>The inference algorithm is the one from the following paper:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.4467&amp;rep=rep1&amp;type=pdf">Variational Inference for Dirichlet Process Mixtures</a>
David Blei, Michael Jordan. Bayesian Analysis, 2006</li>
</ul>
</div></blockquote>
<p>While this paper presents the parts of the inference algorithm that
are concerned with the structure of the dirichlet process, it does not
go into detail in the mixture modeling part, which can be just as
complex, or even more. For this reason we present here a full
derivation of the inference algorithm and all the update and
lower-bound equations. If you&#8217;re not interested in learning how to
derive similar algorithms yourself and you&#8217;re not interested in
changing/debugging the implementation in the scikit this document is
not for you.</p>
<p>The complexity of this implementation is linear in the number of
mixture components and data points. With regards to the
dimensionality, it is linear when using <code class="docutils literal"><span class="pre">spherical</span></code> or <code class="docutils literal"><span class="pre">diag</span></code> and
quadratic/cubic when using <code class="docutils literal"><span class="pre">tied</span></code> or <code class="docutils literal"><span class="pre">full</span></code>. For <code class="docutils literal"><span class="pre">spherical</span></code> or <code class="docutils literal"><span class="pre">diag</span></code>
it is O(n_states * n_points * dimension) and for <code class="docutils literal"><span class="pre">tied</span></code> or <code class="docutils literal"><span class="pre">full</span></code> it
is O(n_states * n_points * dimension^2 + n_states * dimension^3) (it
is necessary to invert the covariance/precision matrices and compute
its determinant, hence the cubic term).</p>
<p>This implementation is expected to scale at least as well as EM for
the Gaussian mixture.</p>
</div>
<div class="section" id="update-rules-for-vb-inference">
<h1>2.1.3.2.2. Update rules for VB inference<a class="headerlink" href="#update-rules-for-vb-inference" title="Permalink to this headline">Â¶</a></h1>
<p>Here the full mathematical derivation of the Variational Bayes update
rules for Gaussian Mixture Models is given. The main parameters of the
model, defined for any class <img class="math" src="../_images/math/5d90819185ae56b9e0d1c08df4bd719b6aa06b03.png" alt="k \in [1..K]"/> are the class
proportion <img class="math" src="../_images/math/34ea3b8fe46ef695984759103d7bcdecd553f321.png" alt="\phi_k"/>, the mean parameters <img class="math" src="../_images/math/e0f4c204f26e9afa4f5bbeda60af5262e624c148.png" alt="\mu_k"/>, the
covariance parameters <img class="math" src="../_images/math/07b943c6dd1061bde470bebfa3e9b5e373216870.png" alt="\Sigma_k"/>, which is characterized by
variational Wishart density, <img class="math" src="../_images/math/24a5a6643f900bf129cdd7a03a6e161d1524ebfc.png" alt="Wishart(a_k, \mathbf{B_k})"/>, where
<img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> is the degrees of freedom, and <img class="math" src="../_images/math/83956e92fcc80dee17fce864543216939a3c9da7.png" alt="B"/> is the
scale matrix. Depending on the covariance parametrization,
<img class="math" src="../_images/math/1a826fd4e37c0ceccbb19369141c92a7c1743e04.png" alt="B_k"/> can be a positive scalar, a positive vector or a Symmetric
Positive Definite matrix.</p>
<div class="section" id="the-spherical-model">
<h2>2.1.3.2.2.1. The spherical model<a class="headerlink" href="#the-spherical-model" title="Permalink to this headline">Â¶</a></h2>
<p>The model then is</p>
<div class="math">
<p><img src="../_images/math/f7c2b84d86cfec64ac6270e494ed9fc10c85fefc.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(1, \alpha_1) \\
\mu_k   &amp;\sim&amp; Normal(0,  \mathbf{I}) \\
\sigma_k &amp;\sim&amp; Gamma(1, 1) \\
z_{i}     &amp;\sim&amp; SBP(\phi) \\
X_t &amp;\sim&amp; Normal(\mu_{z_i}, \frac{1}{\sigma_{z_i}} \mathbf{I})
\end{array}"/></p>
</div><p>The variational distribution we&#8217;ll use is</p>
<div class="math">
<p><img src="../_images/math/3132dd0795e18f7917a8b7a7d66c488807cd1482.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(\gamma_{k,1}, \gamma_{k,2}) \\
\mu_k   &amp;\sim&amp; Normal(\nu_{\mu_k},  \mathbf{I}) \\
\sigma_k &amp;\sim&amp; Gamma(a_{k}, b_{k}) \\
z_{i}     &amp;\sim&amp; Discrete(\nu_{z_i}) \\
\end{array}"/></p>
</div><div class="section" id="the-bound">
<h3>2.1.3.2.2.1.1. The bound<a class="headerlink" href="#the-bound" title="Permalink to this headline">Â¶</a></h3>
<p>The variational bound is</p>
<div class="math">
<p><img src="../_images/math/a83b453e7f4c8baf1ed78ca44473fdadbcccc6d6.png" alt="\begin{array}{rcl}
\log P(X) &amp;\ge&amp;
\sum_k (E_q[\log P(\phi_k)] - E_q[\log Q(\phi_k)]) \\
&amp;&amp;
+\sum_k \left( E_q[\log P(\mu_k)] - E_q[\log Q(\mu_k)] \right) \\
&amp;&amp;
+\sum_k \left( E_q[\log P(\sigma_k)] - E_q[\log Q(\sigma_k)] \right) \\
&amp;&amp;
+\sum_i \left( E_q[\log P(z_i)] - E_q[\log Q(z_i)] \right) \\
&amp;&amp;
+\sum_i E_q[\log P(X_t)]
\end{array}"/></p>
</div><p><strong>The bound for</strong> <img class="math" src="../_images/math/34ea3b8fe46ef695984759103d7bcdecd553f321.png" alt="\phi_k"/></p>
<div class="math">
<p><img src="../_images/math/c80670a81cb5e1cb4cae6fffa705ff534fbb1673.png" alt="\begin{array}{rcl}
E_q[\log Beta(1,\alpha)] - E[\log Beta(\gamma_{k,1},\gamma_{k,2})]
&amp;=&amp;
\log \Gamma(1+\alpha) - \log \Gamma(\alpha) \\ &amp;&amp;
+(\alpha-1)(\Psi(\gamma_{k,2})-\Psi(\gamma_{k,1}+\gamma_{k,2})) \\ &amp;&amp;
- \log \Gamma(\gamma_{k,1}+\gamma_{k,2}) + \log \Gamma(\gamma_{k,1}) +
\log \Gamma(\gamma_{k,2}) \\ &amp;&amp;
-
(\gamma_{k,1}-1)(\Psi(\gamma_{k,1})-\Psi(\gamma_{k,1}+\gamma_{k,2}))
\\ &amp;&amp;
-
(\gamma_{k,2}-1)(\Psi(\gamma_{k,2})-\Psi(\gamma_{k,1}+\gamma_{k,2}))
\end{array}"/></p>
</div><p><strong>The bound for</strong> <img class="math" src="../_images/math/e0f4c204f26e9afa4f5bbeda60af5262e624c148.png" alt="\mu_k"/></p>
<div class="math">
<p><img src="../_images/math/ecf4476d8a4cf73dfff01acc926ba3930a991035.png" alt="\begin{array}{rcl}
&amp;&amp; E_q[\log P(\mu_k)] - E_q[\log Q(\mu_k)] \\
&amp;=&amp;
\int\!d\mu_f q(\mu_f) \log P(\mu_f)
- \int\!d\mu_f q(\mu_f) \log Q(\mu_f)  \\
&amp;=&amp;
- \frac{D}{2}\log 2\pi - \frac{1}{2} ||\nu_{\mu_k}||^2 - \frac{D}{2}
+ \frac{D}{2} \log 2\pi e
\end{array}"/></p>
</div><p><strong>The bound for</strong> <img class="math" src="../_images/math/088138d180d3b322617921d408597bf140875237.png" alt="\sigma_k"/></p>
<p>Here I&#8217;ll use the inverse scale parametrization of the gamma
distribution.</p>
<div class="math">
<p><img src="../_images/math/0c8af1d333d83af425675c09c89969fef6446f45.png" alt="\begin{array}{rcl}
&amp;&amp; E_q[\log P(\sigma_k)] - E_q [\log Q(\sigma_k)] \\ &amp;=&amp;
\log \Gamma (a_k) - (a_k-1)\Psi(a_k) -\log b_k + a_k - \frac{a_k}{b_k}
\end{array}"/></p>
</div><p><strong>The bound for z</strong></p>
<div class="math">
<p><img src="../_images/math/3732fbd017c0ee6f9183827c7b56d5eb37132811.png" alt="\begin{array}{rcl}
&amp;&amp; E_q[\log P(z)] - E_q[\log Q(z)] \\
&amp;=&amp;
\sum_{k} \left(
     \left(\sum_{j=k+1}^K  \nu_{z_{i,j}}\right)(\Psi(\gamma_{k,2})-\Psi(\gamma_{k,1}+\gamma_{k,2}))
 +  \nu_{z_{i,k}}(\Psi(\gamma_{k,1})-\Psi(\gamma_{k,1}+\gamma_{k,2}))
 - \log \nu_{z_{i,k}} \right)
\end{array}"/></p>
</div><p><strong>The bound for</strong> <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/></p>
<p>Recall that there is no need for a <img class="math" src="../_images/math/debe2ff09239bb9b81160d734bd2d97cf7058162.png" alt="Q(X)"/> so this bound is just</p>
<div class="math">
<p><img src="../_images/math/966f59ff910d816a14cb48c2cd22111b0acad5ea.png" alt="\begin{array}{rcl}
E_q[\log P(X_i)] &amp;=&amp; \sum_k \nu_{z_k} \left( - \frac{D}{2}\log 2\pi
+\frac{D}{2} (\Psi(a_k) - \log(b_k))
-\frac{a_k}{2b_k} (||X_i - \nu_{\mu_k}||^2+D) - \log 2 \pi e  \right)
\end{array}"/></p>
</div><p>For simplicity I&#8217;ll later call the term inside the parenthesis <img class="math" src="../_images/math/e43d9cd0fe15ae82f739a60dd2cd5be2c65c1d99.png" alt="E_q[\log P(X_i|z_i=k)]"/></p>
</div>
<div class="section" id="the-updates">
<h3>2.1.3.2.2.1.2. The updates<a class="headerlink" href="#the-updates" title="Permalink to this headline">Â¶</a></h3>
<p><strong>Updating</strong> <img class="math" src="../_images/math/0ebb67342b546ca42a1c634b1ef03c893c4cdedb.png" alt="\gamma"/></p>
<div class="math">
<p><img src="../_images/math/9512ac67b2fd21ff1b8a964cd1c094d856a8a6a2.png" alt="\begin{array}{rcl}
\gamma_{k,1} &amp;=&amp; 1+\sum_i \nu_{z_{i,k}} \\
\gamma_{k,2} &amp;=&amp; \alpha + \sum_i \sum_{j &gt; k} \nu_{z_{i,j}}.
\end{array}"/></p>
</div><p><strong>Updating</strong> <img class="math" src="../_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/></p>
<p>The updates for mu essentially are just weighted expectations of
<img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/> regularized by the prior. We can see this by taking the
gradient of the bound with regards to <img class="math" src="../_images/math/d7b9c9300675caab903407dce0551c0ff4e67663.png" alt="\nu_{\mu}"/> and setting it to zero.
The gradient is</p>
<div class="math">
<p><img src="../_images/math/80c693917f112f7e4f355b60c6ab2156c6247c25.png" alt="\nabla L = -\nu_{\mu_k} + \sum_i \frac{\nu_{z_{i,k}}b_k}{a_k}(X_i + -\nu_{\mu})"/></p>
</div><p>so the update is</p>
<div class="math">
<p><img src="../_images/math/1f875c2bb16e19d860d4930b64488c8ae29826d1.png" alt="\nu_{\mu_k} = \frac{\sum_i \frac{\nu_{z_{i,k}}b_k}{a_k}X_i}{1+\sum_i \frac{\nu_{z_{i,k}}b_k}{a_k}}"/></p>
</div><p><strong>Updating</strong> <img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> <strong>and</strong> <img class="math" src="../_images/math/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png" alt="b"/></p>
<p>For some odd reason it doesn&#8217;t really work when you derive the updates
for a and b using the gradients of the lower bound (terms involving the
<img class="math" src="../_images/math/df391fc64254ea60aabeb28d65a2f40966bc23f9.png" alt="\Psi'"/> function show up and <img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> is hard to isolate).
However, we can use the other formula,</p>
<div class="math">
<p><img src="../_images/math/b13374333e11ec9a17fc2335923b539b83f95af9.png" alt="\log Q(\sigma_k) = E_{v \ne \sigma_k}[\log P] + const"/></p>
</div><p>All the terms not involving <img class="math" src="../_images/math/088138d180d3b322617921d408597bf140875237.png" alt="\sigma_k"/> get folded over into the
constant and we get two terms: the prior and the probability of
<img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/>. This gives us</p>
<div class="math">
<p><img src="../_images/math/978004f28b8d067c083ca7fa3d6c84691e9b2328.png" alt="\log Q(\sigma_k) = -\sigma_k  + \frac{D}{2} \sum_i \nu_{z_{i,k}}\log \sigma_k  - \frac{\sigma_k}{2}\sum_i \nu_{z_{i,k}} (||X_i-\mu_k||^2 + D)"/></p>
</div><p>This is the log of a gamma distribution, with <img class="math" src="../_images/math/62be32aea2502a24649da8d7cc427e3b6bb402a0.png" alt="a_k = 1+\frac{D}{2}\sum_i \nu_{z_{i,k}}"/> and</p>
<div class="math">
<p><img src="../_images/math/43a40383e37e632b96747648969b3e649bf70e79.png" alt="b_k = 1 + \frac{1}{2}\sum_i \nu_{z_{i,k}} (||X_i-\mu_k||^2 + D)."/></p>
</div><p>You can verify this by normalizing the previous term.</p>
<p><strong>Updating</strong> <img class="math" src="../_images/math/84d7271dd9e78c1e05d6c3c6ecb60309ef7dfc73.png" alt="z"/></p>
<div class="math">
<p><img src="../_images/math/6ed3fc944b63925d3ae6b0cefb0e882ddbf4a09a.png" alt="\log \nu_{z_{i,k}} \propto \Psi(\gamma_{k,1}) -
\Psi(\gamma_{k,1} + \gamma_{k,2}) + E_q[\log P(X_i|z_i=k)] +
\sum_{j &lt; k} \left (\Psi(\gamma_{j,2}) -
\Psi(\gamma_{j,1}+\gamma_{j,2})\right)."/></p>
</div></div>
</div>
<div class="section" id="the-diagonal-model">
<h2>2.1.3.2.2.2. The diagonal model<a class="headerlink" href="#the-diagonal-model" title="Permalink to this headline">Â¶</a></h2>
<p>The model then is</p>
<div class="math">
<p><img src="../_images/math/17ad01f253817df5a2df4f7c936197fb719b50a7.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(1, \alpha_1) \\
\mu_k   &amp;\sim&amp; Normal(0,  \mathbf{I}) \\
\sigma_{k,d} &amp;\sim&amp; Gamma(1, 1) \\
z_{i}     &amp;\sim&amp; SBP(\phi) \\
X_t &amp;\sim&amp; Normal(\mu_{z_i}, \bm{\sigma_{z_i}}^{-1})
\end{array}"/></p>
</div><p>Tha variational distribution we&#8217;ll use is</p>
<div class="math">
<p><img src="../_images/math/fe63d8383579ed18ecd1dac79b13c88629a900f0.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(\gamma_{k,1}, \gamma_{k,2}) \\
\mu_k   &amp;\sim&amp; Normal(\nu_{\mu_k},  \mathbf{I}) \\
\sigma_{k,d} &amp;\sim&amp; Gamma(a_{k,d}, b_{k,d}) \\
z_{i}     &amp;\sim&amp; Discrete(\nu_{z_i}) \\
\end{array}"/></p>
</div><div class="section" id="the-lower-bound">
<h3>2.1.3.2.2.2.1. The lower bound<a class="headerlink" href="#the-lower-bound" title="Permalink to this headline">Â¶</a></h3>
<p>The changes in this lower bound from the previous model are in the
distributions of <img class="math" src="../_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> (as there are a lot more <img class="math" src="../_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> s
now) and <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/>.</p>
<p>The bound for <img class="math" src="../_images/math/98960fec06a81f158a2b33b6351e2e87c58861f1.png" alt="\sigma_{k,d}"/> is the same bound for <img class="math" src="../_images/math/088138d180d3b322617921d408597bf140875237.png" alt="\sigma_k"/> and can
be safely omitted.</p>
<p><strong>The bound for</strong> <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/> :</p>
<p>The main difference here is that the precision matrix <img class="math" src="../_images/math/9523e30d2c532ce5ce84b3c272f599e7c7b86953.png" alt="\bm{\sigma_k}"/>
scales the norm, so we have an extra term after computing the
expectation of <img class="math" src="../_images/math/22f4ab6468e7a706bc27b9e5b2769965b9e2991a.png" alt="\mu_k^T\bm{\sigma_k}\mu_k"/>, which is
<img class="math" src="../_images/math/2e6c422c84607919c1467f3f18f81ac44daf48f2.png" alt="\nu_{\mu_k}^T\bm{\sigma_k}\nu_{\mu_k} + \sum_d \sigma_{k,d}"/>. We then
have</p>
<div class="math">
<p><img src="../_images/math/c4963d07ba1773f81a40fd1cbeb2f704bed3ef6a.png" alt="\begin{array}{rcl}
E_q[\log P(X_i)] &amp;=&amp; \sum_k \nu_{z_k} \Big( - \frac{D}{2}\log 2\pi
+\frac{1}{2}\sum_d (\Psi(a_{k,d}) - \log(b_{k,d})) \\
&amp;&amp;
-\frac{1}{2}((X_i - \nu_{\mu_k})^T\bm{\frac{a_k}{b_k}}(X_i - \nu_{\mu_k})+ \sum_d \sigma_{k,d})- \log 2 \pi e  \Big)
\end{array}"/></p>
</div></div>
<div class="section" id="id1">
<h3>2.1.3.2.2.2.2. The updates<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h3>
<p>The updates only chance for <img class="math" src="../_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/> (to weight them with the new
<img class="math" src="../_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/>), <img class="math" src="../_images/math/84d7271dd9e78c1e05d6c3c6ecb60309ef7dfc73.png" alt="z"/> (but the change is all folded into the
<img class="math" src="../_images/math/8c59c344ff9ba0a15e07b4bf3158a3b9d8123072.png" alt="E_q[P(X_i|z_i=k)]"/> term), and the <img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> and <img class="math" src="../_images/math/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png" alt="b"/> variables themselves.</p>
<p><strong>The update for</strong> <img class="math" src="../_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/></p>
<div class="math">
<p><img src="../_images/math/2fb03b7ae1b969f602c838fbcea097ead4e62b96.png" alt="\nu_{\mu_k} = \left(\mathbf{I}+\sum_i \frac{\nu_{z_{i,k}}\mathbf{b_k}}{\mathbf{a_k}}\right)^{-1}\left(\sum_i \frac{\nu_{z_{i,k}}b_k}{a_k}X_i\right)"/></p>
</div><p><strong>The updates for a and b</strong></p>
<p>Here we&#8217;ll do something very similar to the spheric model. The main
difference is that now each <img class="math" src="../_images/math/98960fec06a81f158a2b33b6351e2e87c58861f1.png" alt="\sigma_{k,d}"/> controls only one dimension
of the bound:</p>
<div class="math">
<p><img src="../_images/math/ab4a27deee9f447192a5108b3183569a7a17a565.png" alt="\log Q(\sigma_{k,d}) = -\sigma_{k,d} + \sum_i \nu_{z_{i,k}}\frac{1}{2}\log \sigma_{k,d}
- \frac{\sigma_{k,d}}{2}\sum_i \nu_{z_{i,k}} ((X_{i,d}-\mu_{k,d})^2 + 1)"/></p>
</div><p>Hence</p>
<div class="math">
<p><img src="../_images/math/bfdef1da689380b0ea4fddbc9735060db11c1d7b.png" alt="a_{k,d} = 1 + \frac{1}{2} \sum_i \nu_{z_{i,k}}"/></p>
</div><div class="math">
<p><img src="../_images/math/954bb85f459bf0487481c5a2593f1e4876ad9f8e.png" alt="b_{k,d} = 1 + \frac{1}{2} \sum_i \nu_{z_{i,k}}((X_{i,d}-\mu_{k,d})^2 + 1)"/></p>
</div></div>
</div>
<div class="section" id="the-tied-model">
<h2>2.1.3.2.2.3. The tied model<a class="headerlink" href="#the-tied-model" title="Permalink to this headline">Â¶</a></h2>
<p>The model then is</p>
<div class="math">
<p><img src="../_images/math/cdbdf6d4e4784ed600d2aece0fa151e8f7d2ce4f.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(1, \alpha_1) \\
\mu_k   &amp;\sim&amp; Normal(0,  \mathbf{I}) \\
\Sigma &amp;\sim&amp; Wishart(D, \mathbf{I}) \\
z_{i}     &amp;\sim&amp; SBP(\phi) \\
X_t &amp;\sim&amp; Normal(\mu_{z_i},  \Sigma^{-1})
\end{array}"/></p>
</div><p>Tha variational distribution we&#8217;ll use is</p>
<div class="math">
<p><img src="../_images/math/78386eed362d03c2d2139b43dda8b73b8621f295.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(\gamma_{k,1}, \gamma_{k,2}) \\
\mu_k   &amp;\sim&amp; Normal(\nu_{\mu_k},  \mathbf{I}) \\
\Sigma &amp;\sim&amp; Wishart(a, \mathbf{B}) \\
z_{i}     &amp;\sim&amp; Discrete(\nu_{z_i}) \\
\end{array}"/></p>
</div><div class="section" id="id2">
<h3>2.1.3.2.2.3.1. The lower bound<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h3>
<p>There are two changes in the lower-bound: for <img class="math" src="../_images/math/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png" alt="\Sigma"/> and for <img class="math" src="../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/>.</p>
<p><strong>The bound for</strong> <img class="math" src="../_images/math/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png" alt="\Sigma"/></p>
<div class="math">
<p><img src="../_images/math/55d0ef5caa725a8a4da7a73cb9e37fcf9b15d3c5.png" alt="\begin{array}{rcl}
\frac{D^2}{2}\log 2  + \sum_d \log \Gamma(\frac{D+1-d}{2}) \\
- \frac{aD}{2}\log 2 + \frac{a}{2} \log |\mathbf{B}| + \sum_d \log \Gamma(\frac{a+1-d}{2}) \\
+ \frac{a-D}{2}\left(\sum_d \Psi\left(\frac{a+1-d}{2}\right)
+ D \log 2 + \log |\mathbf{B}|\right) \\
+ \frac{1}{2} a \mathbf{tr}[\mathbf{B}-\mathbf{I}]
\end{array}"/></p>
</div><p><strong>The bound for X</strong></p>
<div class="math">
<p><img src="../_images/math/2f2182c8ad805e1302f7a87961c481c0f59a88ec.png" alt="\begin{array}{rcl}
E_q[\log P(X_i)] &amp;=&amp; \sum_k \nu_{z_k} \Big( - \frac{D}{2}\log 2\pi
+\frac{1}{2}\left(\sum_d \Psi\left(\frac{a+1-d}{2}\right)
+ D \log 2 + \log |\mathbf{B}|\right) \\
&amp;&amp;
-\frac{1}{2}((X_i - \nu_{\mu_k})a\mathbf{B}(X_i - \nu_{\mu_k})+ a\mathbf{tr}(\mathbf{B}))- \log 2 \pi e  \Big)
\end{array}"/></p>
</div></div>
<div class="section" id="id3">
<h3>2.1.3.2.2.3.2. The updates<a class="headerlink" href="#id3" title="Permalink to this headline">Â¶</a></h3>
<p>As in the last setting, what changes are the trivial update for <img class="math" src="../_images/math/84d7271dd9e78c1e05d6c3c6ecb60309ef7dfc73.png" alt="z"/>,
the update for <img class="math" src="../_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/> and the update for <img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> and <img class="math" src="../_images/math/fb542518f2c3338d27234e14932fd1ab5a311d8b.png" alt="\mathbf{B}"/>.</p>
<p><strong>The update for</strong> <img class="math" src="../_images/math/126e84ba38f7dece5f0ad64e929b9588b20f6440.png" alt="\mu"/></p>
<div class="math">
<p><img src="../_images/math/de78c9a0d22071db520afd4ea48fca11fafcc0c1.png" alt="\nu_{\mu_k} = \left(\mathbf{I}+ a\mathbf{B}\sum_i \nu_{z_{i,k}}\right)^{-1}
\left(a\mathbf{B}\sum_i \nu_{z_{i,k}} X_i\right)"/></p>
</div><p><strong>The update for</strong> <img class="math" src="../_images/math/7dd2a5ea01fbd72ad2a58dd1f3d6ecbfde6208a1.png" alt="a"/> <strong>and</strong> <img class="math" src="../_images/math/83956e92fcc80dee17fce864543216939a3c9da7.png" alt="B"/></p>
<p>As this distribution is far too complicated I&#8217;m not even going to try
going at it the gradient way.</p>
<div class="math">
<p><img src="../_images/math/76fea3bf22d002699aa7e9118a874ac5f5b7c89e.png" alt="\log Q(\Sigma) = +\frac{1}{2}\log |\Sigma| - \frac{1}{2} \mathbf{tr}[\Sigma]
+ \sum_i \sum_k \nu_{z_{i,k}} \left( +\frac{1}{2}\log |\Sigma| - \frac{1}{2}((X_i-\nu_{\mu_k})^T\Sigma(X_i-\nu_{\mu_k})+\mathbf{tr}[\Sigma]) \right)"/></p>
</div><p>which non-trivially (seeing that the quadratic form with <img class="math" src="../_images/math/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png" alt="\Sigma"/> in
the middle can be expressed as the trace of something) reduces to</p>
<div class="math">
<p><img src="../_images/math/15bcc0b861d55925d90d2cc1791a3e225726a293.png" alt="\log Q(\Sigma) = +\frac{1}{2}\log |\Sigma| - \frac{1}{2} \mathbf{tr}[\Sigma]
+ \sum_i \sum_k \nu_{z_{i,k}} \left( +\frac{1}{2}\log |\Sigma| - \frac{1}{2}(\mathbf{tr}[(X_i-\nu_{\mu_k})(X_i-\nu_{\mu_k})^T\Sigma]+\mathbf{tr}[I \Sigma]) \right)"/></p>
</div><p>hence this (with a bit of squinting) looks like a wishart with parameters</p>
<div class="math">
<p><img src="../_images/math/4434b502c52698cad6b610c97252d0c43fb8093a.png" alt="a = 2 + D + T"/></p>
</div><p>and</p>
<div class="math">
<p><img src="../_images/math/5bdc70fc16f31e374c9fab80d2fded72994b0319.png" alt="\mathbf{B} = \left(\mathbf{I} + \sum_i \sum_k \nu_{z_{i,k}}(X_i-\nu_{\mu_k})(X_i-\nu_{\mu_k})^T\right)^{-1}"/></p>
</div></div>
</div>
<div class="section" id="the-full-model">
<h2>2.1.3.2.2.4. The full model<a class="headerlink" href="#the-full-model" title="Permalink to this headline">Â¶</a></h2>
<blockquote>
<div>The model then is</div></blockquote>
<div class="math">
<p><img src="../_images/math/e6e2f5e65060d99a8dc3f948fd96ad5fc5c8b002.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(1, \alpha_1) \\
\mu_k   &amp;\sim&amp; Normal(0,  \mathbf{I}) \\
\Sigma_k &amp;\sim&amp; Wishart(D, \mathbf{I}) \\
z_{i}     &amp;\sim&amp; SBP(\phi) \\
X_t &amp;\sim&amp; Normal(\mu_{z_i},  \Sigma_{z,i}^{-1})
\end{array}"/></p>
</div><p>The variational distribution we&#8217;ll use is</p>
<div class="math">
<p><img src="../_images/math/225415e4deb55e55ab8aea056d9d128eb939151a.png" alt="\begin{array}{rcl}
\phi_k   &amp;\sim&amp; Beta(\gamma_{k,1}, \gamma_{k,2}) \\
\mu_k   &amp;\sim&amp; Normal(\nu_{\mu_k},  \mathbf{I}) \\
\Sigma_k &amp;\sim&amp; Wishart(a_k, \mathbf{B_k}) \\
z_{i}     &amp;\sim&amp; Discrete(\nu_{z_i}) \\
\end{array}"/></p>
</div><div class="section" id="id4">
<h3>2.1.3.2.2.4.1. The lower bound<a class="headerlink" href="#id4" title="Permalink to this headline">Â¶</a></h3>
<p>All that changes in this lower bound in comparison to the previous one
is that there are K priors on different <img class="math" src="../_images/math/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png" alt="\Sigma"/> precision matrices
and there are the correct indices on the bound for X.</p>
</div>
<div class="section" id="id5">
<h3>2.1.3.2.2.4.2. The updates<a class="headerlink" href="#id5" title="Permalink to this headline">Â¶</a></h3>
<p>All that changes in the updates is that the update for mu uses only
the proper sigma and the updates for a and B don&#8217;t have a sum over K, so</p>
<div class="math">
<p><img src="../_images/math/33b8c0e2612b409bf78108c87bfb6842c470dd67.png" alt="\nu_{\mu_k} = \left(\mathbf{I}+ a_k\mathbf{B_k}\sum_i \nu_{z_{i,k}}\right)^{-1}
\left(a_k\mathbf{B_k}\sum_i \nu_{z_{i,k}} X_i\right)"/></p>
</div><div class="math">
<p><img src="../_images/math/53f02bd2b9bea6b80f832f4bfc0631572720b7b1.png" alt="a_k = 2 + D + \sum_i \nu_{z_{i,k}}"/></p>
</div><p>and</p>
<div class="math">
<p><img src="../_images/math/7b3d2f4a716311934563c7044cc2596476636d74.png" alt="\mathbf{B} = \left(\left(\sum_i\nu_{z_{i,k}}+1\right)\mathbf{I} + \sum_i  \nu_{z_{i,k}}(X_i-\nu_{\mu_k})(X_i-\nu_{\mu_k})^T\right)^{-1}"/></p>
</div></div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2016, scikit-learn developers (BSD License).
      <a href="../_sources/modules/dp-derivation.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="mixture.html">Previous
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>